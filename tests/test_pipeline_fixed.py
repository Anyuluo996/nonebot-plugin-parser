"""
ä¿®å¤ç‰ˆï¼šå®Œæ•´æµç¨‹æµ‹è¯•ï¼Œæ‰‹åŠ¨å¤„ç†è½¬å‘ç±»å‹
"""

import asyncio
import json
import sys
import os
from pathlib import Path
from datetime import datetime

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))


async def extract_dynamic_info(raw_data: dict, is_orig: bool = False):
    """æ‰‹åŠ¨ä»åŸå§‹æ•°æ®ä¸­æå–åŠ¨æ€ä¿¡æ¯ï¼Œä¸ä¾èµ– msgspec"""

    # ç¡®å®šä½¿ç”¨å“ªä¸ªæ•°æ®
    data = raw_data.get('orig') if is_orig else raw_data.get('item')
    if not data:
        return None

    modules = data.get('modules', {})
    module_author = modules.get('module_author', {})

    # æå–ä½œè€…ä¿¡æ¯
    name = module_author.get('name', '')
    avatar = module_author.get('face', '')

    # æå–æ–‡æœ¬
    text = None
    module_dynamic = modules.get('module_dynamic', {})
    major = module_dynamic.get('major') or {}

    if major and 'opus' in major:
        opus = major['opus']
        summary = opus.get('summary', {})
        text = summary.get('text', '')
    elif major and 'archive' in major:
        archive = major['archive']
        text = archive.get('desc', '')

    # æå–å›¾ç‰‡
    image_urls = []
    if major and 'opus' in major:
        opus = major['opus']
        pics = opus.get('pics', [])
        image_urls = [pic['url'] for pic in pics]
    elif major and 'archive' in major:
        archive = major['archive']
        cover = archive.get('cover')
        if cover:
            image_urls = [cover]

    return {
        "name": name,
        "avatar": avatar,
        "text": text,
        "image_urls": image_urls,
        "type": data.get('type'),
        "is_orig": is_orig
    }


async def test_complete_pipeline_fixed():
    """å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    print("\n" + "â•”" + "="*68 + "â•—")
    print("â•‘" + " "*12 + "Bç«™åŠ¨æ€å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆä¿®å¤ç‰ˆï¼‰" + " "*10 + "â•‘")
    print("â•š" + "="*68 + "â•")
    print(f"\nå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("tests/pipeline_output")
    output_dir.mkdir(exist_ok=True)

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "æ™®é€šå›¾æ–‡åŠ¨æ€",
            "id": 1159504791855955984,
            "expected": {
                "has_text": True,
                "has_images": True,
                "image_count": 1
            }
        },
        {
            "name": "è½¬å‘åŠ¨æ€",
            "id": 1156587796127809560,
            "expected": {
                "has_text": True,
                "has_images": True,
                "image_count": 2
            }
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*70}")
        print(f"æµ‹è¯• {i}/{len(test_cases)}: {test_case['name']}")
        print(f"ID: {test_case['id']}")
        print(f"{'='*70}")

        # 1. è·å– API æ•°æ®
        print(f"\næ­¥éª¤1: è·å– API æ•°æ®...")
        try:
            from bilibili_api.dynamic import Dynamic

            dynamic = Dynamic(test_case['id'])
            raw_data = await dynamic.get_info()

            # ä¿å­˜åŸå§‹æ•°æ®
            raw_file = output_dir / f"test{i}_raw_api.json"
            with open(raw_file, 'w', encoding='utf-8') as f:
                json.dump(raw_data, f, ensure_ascii=False, indent=2)

            print(f"   âœ… API æ•°æ®è·å–æˆåŠŸ")
            print(f"   ğŸ’¾ ä¿å­˜åˆ°: {raw_file.name}")
            print(f"   ç±»å‹: {raw_data['item']['type']}")

        except Exception as e:
            print(f"   âŒ API è·å–å¤±è´¥: {e}")
            continue

        # 2. æ•°æ®æå–ï¼ˆæ‰‹åŠ¨ï¼‰
        print(f"\næ­¥éª¤2: æ•°æ®æå–...")

        # æå– item ä¿¡æ¯
        item_info = await extract_dynamic_info(raw_data, is_orig=False)
        print(f"   item ä¿¡æ¯:")
        print(f"     name: {item_info['name']}")
        print(f"     text: {item_info['text']}")
        print(f"     å›¾ç‰‡æ•°: {len(item_info['image_urls'])}")

        # å¦‚æœæ˜¯è½¬å‘ç±»å‹ï¼Œæå– orig ä¿¡æ¯
        orig_info = None
        if raw_data['item'].get('orig'):
            print(f"\n   æ£€æµ‹åˆ°è½¬å‘ç±»å‹")
            orig_info = await extract_dynamic_info(raw_data, is_orig=True)
            print(f"   åŸåŠ¨æ€ä¿¡æ¯:")
            print(f"     name: {orig_info['name']}")
            print(f"     text: {orig_info['text']}")
            print(f"     å›¾ç‰‡æ•°: {len(orig_info['image_urls'])}")

            # ä½¿ç”¨åŸåŠ¨æ€çš„å†…å®¹
            if orig_info['image_urls'] or orig_info['text']:
                item_info = orig_info
                print(f"   ä½¿ç”¨åŸåŠ¨æ€çš„å†…å®¹")

        # ä¿å­˜è½¬æ¢åçš„æ•°æ®
        converted_file = output_dir / f"test{i}_converted.json"
        converted_data = {
            "item": item_info,
            "has_orig": orig_info is not None,
            "orig": orig_info
        }
        with open(converted_file, 'w', encoding='utf-8') as f:
            json.dump(converted_data, f, ensure_ascii=False, indent=2)

        print(f"\n   âœ… æ•°æ®æå–æˆåŠŸ")
        print(f"   ğŸ’¾ ä¿å­˜åˆ°: {converted_file.name}")
        print(f"   æœ€ç»ˆæ–‡æœ¬: {item_info['text']}")
        print(f"   æœ€ç»ˆå›¾ç‰‡æ•°: {len(item_info['image_urls'])}")

        # 3. ä¸‹è½½å›¾ç‰‡
        print(f"\næ­¥éª¤3: ä¸‹è½½å›¾ç‰‡...")
        try:
            import httpx

            downloaded_images = []

            for j, image_url in enumerate(item_info['image_urls']):
                print(f"   ä¸‹è½½å›¾ç‰‡{j+1}...")

                # ä¸‹è½½å›¾ç‰‡
                async with httpx.AsyncClient(timeout=30) as client:
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                        "Referer": "https://www.bilibili.com",
                    }

                    response = await client.get(image_url, headers=headers)
                    if response.status_code == 200:
                        # ä¿å­˜å›¾ç‰‡
                        image_ext = image_url.split('.')[-1] if '.' in image_url.split('?')[0] else 'jpg'
                        image_filename = f"test{i}_image{j+1}.{image_ext}"
                        image_path = output_dir / image_filename

                        with open(image_path, 'wb') as f:
                            f.write(response.content)

                        downloaded_images.append({
                            "index": j + 1,
                            "filename": image_filename,
                            "path": str(image_path),
                            "size": len(response.content),
                            "url": image_url
                        })

                        print(f"     âœ… ä¸‹è½½æˆåŠŸ: {image_filename} ({len(response.content)} bytes)")
                    else:
                        print(f"     âŒ ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")

            # ä¿å­˜ä¸‹è½½ä¿¡æ¯
            download_file = output_dir / f"test{i}_download_info.json"
            with open(download_file, 'w', encoding='utf-8') as f:
                json.dump(downloaded_images, f, ensure_ascii=False, indent=2)

            print(f"\n   âœ… å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len(downloaded_images)}/{len(item_info['image_urls'])}")
            print(f"   ğŸ’¾ ä¿¡æ¯ä¿å­˜åˆ°: {download_file.name}")

        except Exception as e:
            print(f"   âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

        # 4. è¯Šæ–­ç»“æœ
        print(f"\næ­¥éª¤4: è¯Šæ–­ç»“æœ...")
        issues = []

        if not item_info['text']:
            issues.append("âš ï¸  æ–‡æœ¬ä¸ºç©º")
        else:
            print(f"   âœ… æ–‡æœ¬: {item_info['text'][:50]}...")

        if not item_info['image_urls']:
            issues.append("âš ï¸ ï¸ å›¾ç‰‡åˆ—è¡¨ä¸ºç©º")
        else:
            print(f"   âœ… å›¾ç‰‡æ•°: {len(item_info['image_urls'])}")

        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸ
        expected = test_case['expected']
        actual_has_text = item_info['text'] is not None
        actual_has_images = len(item_info['image_urls']) > 0

        if actual_has_text != expected['has_text']:
            issues.append(f"âš ï¸  æ–‡æœ¬ä¸é¢„æœŸä¸ç¬¦: {actual_has_text} vs {expected['has_text']}")

        if len(item_info['image_urls']) != expected['image_count']:
            issues.append(f"âš ï¸  å›¾ç‰‡æ•°é‡ä¸ç¬¦: {len(item_info['image_urls'])} vs {expected['image_count']}")

        if not issues and downloaded_images:
            print(f"   âœ… æµ‹è¯•é€šè¿‡ï¼æ‰€æœ‰æ£€æŸ¥é¡¹éƒ½ç¬¦åˆé¢„æœŸ")
        elif issues:
            print(f"   âš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues:
                print(f"     {issue}")
        else:
            print(f"   âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸‹è½½ä¿¡æ¯")

    # ç”Ÿæˆæ€»ç»“
    print(f"\n{'='*70}")
    print("ç”Ÿæˆæ€»ç»“")
    print(f"{'='*70}\n")

    print(f"âœ… æµ‹è¯•å®Œæˆï¼Œæ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}/")
    print(f"\næ–‡ä»¶è¯´æ˜:")
    print(f"  ğŸ“„ test*_raw_api.json - åŸå§‹ API æ•°æ®")
    print(f"  ğŸ“„ test*_converted.json - æå–çš„æ•°æ®ï¼ˆåŒ…å« item å’Œ origï¼‰")
    print(f"  ğŸ“„ test*_download_info.json - å›¾ç‰‡ä¸‹è½½ä¿¡æ¯")
    print(f"  ğŸ–¼ï¸  test*_image*.jpg - ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶")
    print(f"  ğŸ“Š _report.json - æ€»ç»“æŠ¥å‘Š")
    print(f"\nè¯·æ£€æŸ¥è¿™äº›æ–‡ä»¶ï¼Œç‰¹åˆ«æ˜¯ï¼š")
    print(f"  1. test*_converted.json - ç¡®è®¤æ•°æ®æ˜¯å¦æ­£ç¡®æå–")
    print(f"  2. test*_image*.jpg - æŸ¥çœ‹ä¸‹è½½çš„å›¾ç‰‡æ˜¯å¦æ­£å¸¸")
    print(f"  3. å¦‚æœè½¬å‘åŠ¨æ€çš„å›¾ç‰‡å’Œæ–‡æœ¬æ­£ç¡®ï¼Œè¯´æ˜ä»£ç é€»è¾‘æ­£ç¡®")

    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


async def main():
    await test_complete_pipeline_fixed()

    print(f"\n" + "="*70)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
